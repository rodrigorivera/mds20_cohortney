# -*- coding: utf-8 -*-
"""cohortney.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d5JLOrG-lVlHdlg4gGjXi9p4ZYQafsDx
"""

from pathlib import Path
from zipfile import ZipFile
import pandas as pd
import torch
import random
import numpy as np
from matplotlib import pyplot as plt
from tqdm import tqdm, trange
import os
import re



def load_data(data_dir, maxsize=None, maxlen=-1, ext='txt', datetime=True):
    """
    Loads the sequences saved in the given directory.

    Args:
        data_dir    (str, Path) - directory containing sequences
        maxsize     (int)       - maximum number of sequences to load
        maxlen      (int)       - maximum length of sequence, the sequences longer than maxlen will be truncated
        ext         (str)       - extension of files in data_dir directory
        datetime    (bool)      - variable meaning if time values in files are represented in datetime format

    Returns:
        ss          (List(torch.Tensor))    - list of torch.Tensor containing sequences. Each tensor has shape (L, 2) and represents event sequence 
                                                as sequence of pairs (t, c). t - time, c - event type.
        Ts          (torch.Tensor)          - tensor of right edges T_n of interavls (0, T_n) in which point processes realizations lie.
        class2idx   (Dict)                  - dict of event types and their indexes
        user_list   (List(Dict))            - representation of sequences siutable for Cohortny
             
    """

    s = []
    classes = set()
    nb_files = 0
    for file in os.listdir(data_dir):
        if file.endswith(f'.{ext}') and re.sub(fr'.{ext}', '', file).isnumeric():
            if maxsize is None or nb_files <= maxsize:
                nb_files += 1
            else:
                break
            df = pd.read_csv(Path(data_dir, file))
            classes = classes.union(set(df['event'].unique()))
            if datetime:
                df['time'] = pd.to_datetime(df['time'])
                df['time'] = (df['time'] - df['time'][0]) / np.timedelta64(1,'D')
            if maxlen > 0:
                df = df.iloc[:maxlen]

            s.append(df)

    classes = list(classes)
    class2idx = {clas: idx for idx, clas in enumerate(classes)}

    ss, Ts = [], []
    user_list = []
    for i, df in enumerate(s):
      user_dict = dict()
      if s[i]['time'].to_numpy()[-1] < 0:
             continue
      s[i]['event'].replace(class2idx, inplace=True)
      for  event_type in class2idx.values():
          dat = s[i][s[i]['event'] == event_type]
          user_dict[event_type] = dat['time'].to_numpy()
      user_list.append(user_dict)


      st = np.vstack([s[i]['time'].to_numpy(), s[i]['event'].to_numpy()])
      tens = torch.FloatTensor(st.astype(np.float32)).T
      
      if maxlen > 0:
          tens = tens[:maxlen]
      ss.append(tens)
      Ts.append(tens[-1, 0])

    Ts = torch.FloatTensor(Ts)

    return ss, Ts, class2idx, user_list 

def dict_to_pk(dict):
  pk = []
  for ss in dict.values():
    pk.extend(ss)
  return pk

#transforming data to the array taking into account an event type
def sep_hawkes_proc(user_list, event_type):
  sep_seqs = []
  for user_dict in user_list:
    sep_seqs.append(np.array(user_dict[event_type], dtype = np.float32))

  return sep_seqs

#transforming data to the tensor without putting attention at event type
def hawkes_process_wo_event_types(ss):
  sep_seqs = torch.tensor([],dtype=torch.float32)
  for i in range(len(ss)):

      sep_seqs = torch.cat((sep_seqs, torch.unsqueeze(ss[i].T[0],0)), 0)
  return sep_seqs

def fws(p, t1, t2):
  n = sum(list(map(int, (p >= float(t1)) & (p <= float(t2)))))
  return min(int(np.log2(n+1)), 9)

def multiclass_fws_array(user_dict, time_partition):
  fws = []
  for event, subseq in user_dict.items():
    arr = fws_numerical_array(subseq, time_partition)
    fws.append(arr)
  return fws

def fws_numerical_array(p, array):
  fws_array = []
  for i in range(1, len(array)):
    fws_array.append(fws(p, array[i-1], array[i]))
  # fws_array = tuple(fws_array)
  return fws_array

def arr_func(events, T_j, delta_T, fws_func):
  events_fws = dict()
  for p_k in events:
    
    
    fws_val =  fws_func(p_k, delta_T)
    # fws_val = hs.join([str(el) for el in fws_val])
    if type(p_k) == dict:
      # print('check')
      p_k = dict_to_pk(p_k)
    p_k1 = tuple(p_k)
    if p_k1 not in events_fws.keys():
      events_fws[p_k1] = []
      events_fws[p_k1].append(fws_val)
    else:
      # print('here')
      events_fws[p_k1].append(fws_val)

  array = []
  for val in events_fws.values():
    # print(val)
    array.append(list(val[0]))
  return array, events_fws

def events_tensor(events_fws):
  keys_list = list(events_fws.keys())
  full_tensor_batch = torch.tensor([], dtype=torch.float32)
  for key in keys_list:
  # events_fws.values():
    
    ten = torch.tensor(events_fws[key]).unsqueeze(0)
    # print(ten.shape)
    if ten.shape[1] == 1:
      full_tensor_batch = torch.cat((full_tensor_batch, ten), dim=0)
    else:
      for i in range(ten.shape[1]):
        ten2 = ten[:,i , :].unsqueeze(0)
        full_tensor_batch = torch.cat((full_tensor_batch, ten2), dim=0)
  if len(full_tensor_batch.shape) == 4:
    full_tensor_batch = full_tensor_batch.squeeze(axis=1)
  return full_tensor_batch



