{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-DTW\n",
    "Soft Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![picture](https://drive.google.com/uc?export=view&id=1Xmfdpc8JlYsn_erpDiCQs778YNqAEpvY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta$ - cost matrix, $A$ - alignment matrix, $\\gamma$ - smoothing parameter\n",
    "\n",
    "$\\min^\\gamma = (\\max_j z_j) + \\log \\sum_i e^{z_i âˆ’ \\max_j z_j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Soft_DTW(nn.Module):\n",
    "    def __init__(self, gamma=1.0, norm=False):\n",
    "        super(Soft_DTW, self).__init__()\n",
    "        self.norm = norm\n",
    "        self.gamma = gamma\n",
    "        self.dtw = Func_SDTW.apply\n",
    "\n",
    "    def dist_matrix(self, x, y):\n",
    "        dim0, dim1, dim2 = x.size(1), y.size(1), x.size(2)\n",
    "        x = x.unsqueeze(2).expand(-1, dim0, dim1, dim2)\n",
    "        y = y.unsqueeze(1).expand(-1, dim0, dim1, dim2)\n",
    "\n",
    "        return torch.pow(x - y, 2).sum(3)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        # preparations\n",
    "        assert len(x.shape) == len(y.shape)\n",
    "        unsqueezed = False\n",
    "        if len(x.shape) < 3:\n",
    "            x = x.unsqueeze(0)\n",
    "            y = y.unsqueeze(0)\n",
    "            unsqueezed = True\n",
    "        \n",
    "        # find distance matrix between x, y\n",
    "        D_xy = self.dist_matrix(x, y)\n",
    "        \n",
    "        # calc soft-DTW on martix D\n",
    "        out_xy = self.dtw(D_xy, self.gamma)\n",
    "        \n",
    "        # normalize if needed\n",
    "        if self.norm:\n",
    "            D_xx = self.dist_matrix(x, x)\n",
    "            out_xx = self.dtw(D_xx, self.gamma)\n",
    "            D_yy = self.dist_matrix(y, y)\n",
    "            out_yy = self.dtw(D_yy, self.gamma)\n",
    "            output = out_xy - (out_xx + out_yy)/2\n",
    "        else:\n",
    "            output = out_xy\n",
    "        \n",
    "        return output.squeeze(0) if unsqueezed else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func_SDTW(Function):\n",
    "    def forward(ctx, D, gamma):\n",
    "        R = torch.Tensor(sdtw_forward(D, gamma)).to(D.device)\n",
    "        ctx.save_for_backward(D, R, gamma)\n",
    "        \n",
    "        return R[:, -2, -2]\n",
    "\n",
    "    def backward(ctx, grad_output):\n",
    "        D, R, gamma = ctx.saved_tensors\n",
    "        E = torch.Tensor(sdtw_backward(D, R, gamma.item())).to(grad_output.device)\n",
    "        \n",
    "        return grad_output.view(-1, 1, 1).expand_as(E)*E, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdtw_forward(D, gamma):\n",
    "    dim0, dim1, dim2 = D.shape\n",
    "    R = torch.ones((dim0, dim1+2, dim2+2))*np.inf\n",
    "    R[:, 0, 0] = 0\n",
    "    \n",
    "    for i in range(dim0):\n",
    "        for j in range(1, dim2+1):\n",
    "            for k in range(1, dim1+1):\n",
    "                z0 = -R[i, k-1, j-1]/gamma\n",
    "                z1 = -R[i, k-1, j]/gamma\n",
    "                z2 = -R[i, k, j-1]/gamma\n",
    "                zmax = max(z0, z1, z2)\n",
    "                zsum = torch.exp(z0 - zmax) + torch.exp(z1 - zmax) + torch.exp(z2 - zmax)\n",
    "                softmin = -(torch.log(zsum) + zmax)*gamma\n",
    "                R[i, k, j] = D[i, k-1, j-1] + softmin      \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdtw_backward(D, R, gamma):\n",
    "    dim0, dim1, dim2 = D.size()\n",
    "    D1 = E = torch.zeros((dim0, dim1+2, dim2+2))\n",
    "    D1[:, 1:N+1, 1:M+1] = D\n",
    "    E[:, -1, -1] = 1\n",
    "    R[:, : , -1] = -np.inf\n",
    "    R[:, -1, :] = -np.inf\n",
    "    R[:, -1, -1] = R[:, -2, -2]\n",
    "    \n",
    "    for i in range(dim0):\n",
    "        for j in range(dim2, 0, -1):\n",
    "            for k in range(dim1, 0, -1):\n",
    "                a = torch.exp((R[i, k+1, j] - R[i, k, j] - D1[i, k+1, j])/gamma)\n",
    "                b = torch.exp((R[i, k, j+1] - R[i, k, j] - D1[i, k, j+1])/gamma)\n",
    "                c = torch.exp((R[i, k+1, j+1] - R[i, k, j] - D1[i, k+1, j+1])/gamma)\n",
    "                E[i, k, j] = E[i, k+1, j]*a + E[i, k, j+1]*b + E[i, k+1, j+1]*c\n",
    "    \n",
    "    return E[:, 1:N+1, 1:M+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(5, 4)\n",
    "b = torch.randn(6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(37.0379)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = Soft_DTW(gamma=1.0, norm=True)\n",
    "loss = criterion(a, b)\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
